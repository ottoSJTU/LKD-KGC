{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe8cf8-0958-4dda-974c-2a320cb58514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "from utils.utils import TagTree, save_dict_to_file, load_dict_from_file\n",
    "from utils.logger import setup_logger\n",
    "import utils.vector_db as vector_db\n",
    "\n",
    "import logging\n",
    "import Levenshtein\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import jieba\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import hashlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096443a8-c49f-4fd6-ae04-f41d6a1beaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\"\n",
    "DEFAULT_MODEL = \"Meta-Llama-3-1-70B-Instruct\"\n",
    "DATA_PATH = \"your_document_path\"\n",
    "OUTPUT_PATH = \"./output/test_data\"\n",
    "\n",
    "if not os.path.exists(DATA_PATH): exit(0)\n",
    "if not os.path.exists(OUTPUT_PATH): os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef76951-0885-41d5-87ce-a11a67f0d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "log_file_path = f'{OUTPUT_PATH}/entity.log'\n",
    "logger = setup_logger('logger', log_file_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518df2e-6d1f-4a51-8aee-1e510b4c850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device=\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6dfb3-32ee-4a88-a48d-33e19a767dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "import time\n",
    "import requests\n",
    "\n",
    "#解决限流问题\n",
    "def traffic_limit(max_qpm, max_qps):\n",
    "    def decorator(func):\n",
    "        qps_stack = []\n",
    "        qpm_stack = []\n",
    "\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            now = time.time()\n",
    "            while qps_stack and now - qps_stack[0] > 1:\n",
    "                qps_stack.pop(0)\n",
    "            while qpm_stack and now - qpm_stack[0] > 60:\n",
    "                qpm_stack.pop(0)        \n",
    "                \n",
    "            # 检查当前调用是否超过了限制\n",
    "            if len(qps_stack) >= max_qps:\n",
    "                print(\"waiting for QPS control\")\n",
    "                time.sleep(1.1)                \n",
    "            if len(qpm_stack) >= max_qpm:\n",
    "                try:\n",
    "                    sleep_time = qpm_stack[0]+60-time.time()+5\n",
    "                    print(f\"waiting for QPM Control: {sleep_time}s\")\n",
    "                    time.sleep(sleep_time)\n",
    "                except: time.sleep(60)\n",
    "                \n",
    "            qps_stack.append(time.time())\n",
    "            qpm_stack.append(time.time())\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@traffic_limit(20, 1)\n",
    "def request_model(prompt, context=None, model=DEFAULT_MODEL, temperature=0.1, max_tokens=30000):\n",
    "    prompt = prompt[:max_tokens]\n",
    "    msgs = [{\"role\":\"system\",   \"content\": \"The user may ask questions in Chinese or English. Please strictly follow the user’s required output format.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}]\n",
    "    client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "    \n",
    "    if context is not None:\n",
    "        context += msgs\n",
    "        msgs = context\n",
    "    \n",
    "    extend_fields= {\"top_k\": 1, \"max_new_tokens\":1000}\n",
    "    try:\n",
    "        #print(msgs)\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=msgs,\n",
    "            stream=False,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            timeout=Timeout(120, 20),\n",
    "            top_p=0.8,\n",
    "            extend_fields=extend_fields)\n",
    "        \n",
    "        if context is not None: context.append({\"role\":\"assistant\", \"content\": response.choices[0].message.content})\n",
    "        #print(response.error_code)\n",
    "        if response.error_code is None:\n",
    "            resp_text = json.dumps(response.choices[0].message.content, ensure_ascii=False)\n",
    "            think_pattern = r'<think>.*?</think>'\n",
    "            clean_text = re.sub(think_pattern, '', resp_text)\n",
    "            return clean_text\n",
    "        else:\n",
    "            logger.error( f'request fail: {response.error_code}')\n",
    "            raise\n",
    "            \n",
    "    except: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683442f-fdbd-4ce5-aa70-21cf35e98841",
   "metadata": {},
   "outputs": [],
   "source": [
    "###util functions\n",
    "def lsdir(dir_path):\n",
    "    dir_subs = os.listdir(dir_path)\n",
    "    return [item for item in dir_subs if not item.startswith('.')]\n",
    "\n",
    "def count_file_num(KB_dir):\n",
    "    total_files = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(KB_dir):\n",
    "        dirnames[:] = [d for d in dirnames if not d.startswith('.')]\n",
    "        total_files += len(filenames)\n",
    "    return total_files\n",
    "\n",
    "def check_key_existence(KB_dir, json_dir):\n",
    "    json_file = load_dict_from_file(json_dir)\n",
    "    for dirpath, dirnames, filenames in os.walk(KB_dir):\n",
    "        dirnames[:] = [d for d in dirnames if not d.startswith('.')]\n",
    "        for fname in filenames:\n",
    "            file_path = os.path.join(dirpath, fname)\n",
    "            if os.path.abspath(file_path) not in json_file.keys(): print(file_path)\n",
    "    return\n",
    "\n",
    "def find_best_match(a_element, b_list):\n",
    "    best_match = None\n",
    "    best_similarity = -1  \n",
    "    for b_element in b_list:\n",
    "        distance = Levenshtein.distance(a_element, b_element)\n",
    "        similarity = 1 - (distance / max(len(a_element), len(b_element)))\n",
    "        \n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_match = b_element\n",
    "            \n",
    "    return best_match\n",
    "\n",
    "def parse_model_resp(resp_type):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            resp = func(*args, **kwargs)\n",
    "            \n",
    "            def extract_formatted_data(text):\n",
    "                try:\n",
    "                    sections = text.split('###')\n",
    "                except: raise\n",
    "                    \n",
    "                if len(sections) < 2:\n",
    "                    raise ZeroDivisionError\n",
    "                \n",
    "                formatted_text = sections[1].replace('\\\\n', '').replace('\\r', '')\n",
    "                formatted_text = formatted_text.replace(\"\\\\\", \"\")\n",
    "                formatted_text = formatted_text.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"：\",\":\").replace(\"，\", \",\").replace(\"；\", \";\").replace(\";\\\"\", \"\\\"\")\n",
    "                token_count = len(word_tokenize(formatted_text))\n",
    "                if token_count > 3000:  return \"\"\n",
    "                #print(formatted_text)\n",
    "                if resp_type == \"list\":\n",
    "                    try:\n",
    "                        def ensure_list_quotes(input_str):\n",
    "                            input_str = input_str.strip()[1:-1]\n",
    "                            elements = [element.strip() for element in input_str.split(',')]\n",
    "                            \n",
    "                            quoted_elements = [\n",
    "                                element if (element.startswith('\"') and element.endswith('\"')) or \\\n",
    "                                           (element.startswith(\"'\") and element.endswith(\"'\"))\n",
    "                                else f'\"{element}\"'\n",
    "                                for element in elements\n",
    "                            ]\n",
    "                            \n",
    "\n",
    "                            result = '[' + ','.join(quoted_elements) + ']'\n",
    "                            return result\n",
    "                        result = ast.literal_eval(ensure_list_quotes(formatted_text))\n",
    "                        return result\n",
    "                    except (SyntaxError, ValueError) as e:\n",
    "                        logger.error(\"Error parsing model response:\", e, formatted_text)\n",
    "                        raise\n",
    "                        \n",
    "                elif resp_type == \"str\":\n",
    "                    return formatted_text.replace(\"\\\"\",\"\").replace(\"\\'\",\"\")\n",
    "                    \n",
    "                elif resp_type == \"original_str\":\n",
    "                    return formatted_text\n",
    "                    \n",
    "                elif resp_type == \"dict\":\n",
    "                    try:\n",
    "                        #print(formatted_text)\n",
    "                        #formatted_text =  re.sub(r'^\"(.*)\"$', r'\\'\\1\\'', formatted_text)\n",
    "                        result = ast.literal_eval(formatted_text)\n",
    "                        return result\n",
    "                    except Exception as e:\n",
    "                        logger.error(\"Error parsing model response:\", e, formatted_text)\n",
    "                        raise\n",
    "\n",
    "            # 调用内部函数并返回结果\n",
    "            parsed_response = extract_formatted_data(resp)\n",
    "            logger.info(parsed_response)\n",
    "            return parsed_response\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "@parse_model_resp(resp_type=\"list\")\n",
    "def sort_dir(dir_path, context=None):\n",
    "    dir_subs = lsdir(dir_path)    \n",
    "    prompt = f\"\"\"\n",
    "    Currently, there is a document represented as a list: {dir_subs}\n",
    "    -----------------------------------\n",
    "    If you need to read these documents in sequence to build a knowledge graph, and when processing subsequent documents, you can refer to the previous ones, what order do you think would best utilize the existing knowledge?\n",
    "    Note: If there is only one document, return the original document as is. If there are multiple documents, you should prioritize reading summary or overview documents first, followed by documents that provide specific background knowledge, and finally documents that involve specific operational procedures.\n",
    "    If there is historical dialogue context, you need to consider why the previous responses in the conversation were incorrect and then answer according to the requirements.\n",
    "    -----------------------------------\n",
    "    Provide your reasoning process and return the following format with priorities sorted from highest to lowest, marked by ###:\n",
    "    Note only the list should be marked by ###, but not the reasoning process.\n",
    "    ###\n",
    "    [Returned priority list]\n",
    "    ###\n",
    "    Example output format:\n",
    "    ###['A','B']###\n",
    "    \"\"\"\n",
    "    logger.info(prompt)\n",
    "    resp =  request_model(prompt, context)\n",
    "    #print(resp)\n",
    "    return resp\n",
    "\n",
    "\n",
    "def refine_subnames(base, dir_subs):\n",
    "    true_subnames = os.listdir(base)\n",
    "    def replace_with_best_matches(A, B):\n",
    "        replaced_list = [find_best_match(a, B) for a in A]\n",
    "        return replaced_list\n",
    "        \n",
    "    return replace_with_best_matches(dir_subs, true_subnames)\n",
    "\n",
    "@parse_model_resp(resp_type=\"dict\")\n",
    "def test_format(s):\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61caa1bc-f462-451d-ab2a-9bbb4fe1463f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@parse_model_resp(resp_type=\"original_str\")\n",
    "def summarize_no_ref(doc_path):\n",
    "    title = os.path.basename(doc_path)\n",
    "    with open(doc_path, \"r\") as f:\n",
    "        doc_content = f.read()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        ----------------Task Requirements-------------------\n",
    "        \n",
    "        Summarize the content of the current input document in one sentence, no more than 100 words. Answer in English.\n",
    "        \n",
    "        ----------------Current Input Document-------------------\n",
    "        \n",
    "        Title: {title}\n",
    "        \n",
    "        Content: {doc_content}\n",
    "        \n",
    "        ---------------Output Format Requirements--------------------\n",
    "        \n",
    "        Return a string marked with ### at the beginning and end, and the string must not contain single or double quotes. The format is as follows:\n",
    "        ###\n",
    "        Summary content\n",
    "        ###\n",
    "    \"\"\"\n",
    "    logger.info(prompt)\n",
    "    return request_model(prompt)\n",
    "\n",
    "\n",
    "#doc:待总结文档path\n",
    "#ref_sums: {path: sum, }\n",
    "def summarize_with_ref(doc_path, vecDB_path, max_context=20):\n",
    "    title = os.path.basename(doc_path)\n",
    "    with open(doc_path, \"r\") as f:\n",
    "        doc_content = f.read()\n",
    "        #doc_content = \"\\n\".join(line for line in doc_content.splitlines() if line.strip() != \"\")\n",
    "    \n",
    "    \n",
    "    #每次从tmp vector DB检索\n",
    "    if not os.path.exists(vecDB_path): ref_sums=[]\n",
    "    else: ref_sums = vector_db.query(doc_content, max_context, embed_model, vecDB_path)\n",
    "        \n",
    "    ref_context = \"\"\n",
    "    for i, ref in enumerate(ref_sums):\n",
    "        #print(ref_path)\n",
    "        ref_context += f\"{i+1}. {ref} \\n\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    ----------------Task Requirements-------------------\n",
    "    \n",
    "    Briefly summarize the content of the current input document, answering in the same language as the input text.\n",
    "    \n",
    "    ---------------Output Format Requirements--------------------\n",
    "    \n",
    "    Return a string that must not contain single or double quotes. The format is as follows:\n",
    "\n",
    "    ----------------Current Input Document-------------------\n",
    "    \n",
    "    Title: {title}\n",
    "    \n",
    "    Content: {doc_content}\n",
    "    \n",
    "    ---------------Background Knowledge-------------------\n",
    "    The input document is located in a subdirectory of the following document, and all entities mentioned within the document are within the context of the background knowledge. The background knowledge may include domain introductions, terminology definitions, operational methods, etc. Please consider this background knowledge when summarizing the input document. Note that you should summarize the current input document, referencing the background knowledge, rather than treating the background knowledge as the main document.\n",
    "    Background knowledge:\n",
    "    {ref_context}\n",
    "    \"\"\"\n",
    "    logger.info(prompt)\n",
    "    return request_model(prompt)\n",
    "\n",
    "\n",
    "#输入文件夹根目录， 保存目录, 总结向量存储目录\n",
    "def summarize_KB_docs(KB_dir, save_dir, max_context=20):\n",
    "    total_files = 0\n",
    "    accessed_files = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(KB_dir):\n",
    "        dirnames[:] = [d for d in dirnames if not d.startswith('.')]\n",
    "        total_files += len(filenames)\n",
    "    \n",
    "    docs_summary = {}\n",
    "    vecDB_path = os.path.join(os.path.dirname(save_dir), \"summary_faiss_index.bin\")\n",
    "    if os.path.exists(vecDB_path): os.remove(vecDB_path)\n",
    "    vecDB_dir = os.path.dirname(vecDB_path)\n",
    "    if not os.path.exists(vecDB_dir):\n",
    "        os.makedirs(vecDB_dir)\n",
    "        \n",
    "    def do_summarize(base_dir, filetype=\".md\"):\n",
    "        nonlocal accessed_files\n",
    "        def equal_to_list(ele, lst):\n",
    "            if not isinstance(ele, list):\n",
    "                return False\n",
    "            return all(ele.count(x) == lst.count(x) for x in set(lst))\n",
    "            \n",
    "        if lsdir(base_dir) == []: return \n",
    "        logger.info(\"\\n-------------------\")\n",
    "        logger.info(f\"base dir: {base_dir}\")\n",
    "        sort_context = []\n",
    "        dir_subs = sort_dir(base_dir, sort_context) \n",
    "        \n",
    "        #保证排序结果合法\n",
    "        max_retry = 5\n",
    "        while not(equal_to_list(dir_subs, lsdir(base_dir))) and max_retry>0:\n",
    "            logger.error(f\"invalid sorted directory: {dir_subs}, retry...\")\n",
    "            dir_subs = sort_dir(base_dir, sort_context)\n",
    "            logger.info(f\"newly sorted dir: {dir_subs}\")     \n",
    "            dir_subs = refine_subnames(base_dir, dir_subs)\n",
    "            logger.info(f\"refined sorted dir: {dir_subs}\")\n",
    "            max_retry -= 1\n",
    "\n",
    "        if not(equal_to_list(dir_subs, lsdir(base_dir))): \n",
    "            logger.error(f\"sorting failed: {base_dir}\")\n",
    "            return\n",
    "    \n",
    "        logger.info(f\"final sorted dir: {dir_subs}\")  \n",
    "        for sub in dir_subs:\n",
    "            abs_path = os.path.abspath(os.path.join(base_dir, sub))\n",
    "            print(abs_path)\n",
    "            if not os.path.exists(abs_path):\n",
    "                logger.error(f\"非法路径：{abs_path}\")\n",
    "                continue\n",
    "                \n",
    "            if os.path.isfile(abs_path) and abs_path.endswith(filetype):\n",
    "                try:\n",
    "                    logger.info(abs_path)\n",
    "                    logger.info(f\"ref_docs: { [os.path.basename(docname) for docname in docs_summary.keys()] }\")\n",
    "                    single_doc_summary = summarize_with_ref(abs_path, vecDB_path, max_context)\n",
    "                    print(single_doc_summary)\n",
    "                    docs_summary[abs_path] = single_doc_summary\n",
    "                    vector_db.index_text(single_doc_summary, f\"title：{os.path.basename(abs_path)}， content : {single_doc_summary}\", embed_model, vecDB_path)\n",
    "                    accessed_files += 1\n",
    "                    print(f\"{accessed_files}/{total_files}\")\n",
    "                except:\n",
    "                    logger.error(f\"error processing {abs_path}\")\n",
    "                    print(f\"error processing {abs_path}\")\n",
    "                    continue\n",
    "            elif os.path.isdir(abs_path):\n",
    "                do_summarize(abs_path)  \n",
    "        return\n",
    "        \n",
    "    do_summarize(KB_dir)\n",
    "    save_dict_to_file(docs_summary, save_dir)\n",
    "    #os.remove(vecDB_path)\n",
    "    print(\"done\")\n",
    "    return docs_summary\n",
    "    \n",
    "\n",
    "summarize_KB_docs(DATA_PATH, f\"{OUTPUT_PATH}/docs_summary.json\", 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fea617-bbf5-4454-b8bb-5aa0700db811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags: {tag:weight}(weighted) or [tag,](not weighted)\n",
    "def kmeans_clust(tags_with_weight, weighted=False, visualize=False):\n",
    "    def find_origin_tag(embed, tags, embeddings):\n",
    "        cosine_similarities = cosine_similarity([embed], embeddings)[0] \n",
    "        #print(embed.shape, embeddings.shape, cosine_similarities.shape)\n",
    "        most_similar_index = np.argmax(cosine_similarities)\n",
    "        origin_tag = tags[most_similar_index]\n",
    "        return origin_tag\n",
    "        \n",
    "    if weighted:\n",
    "        tags = list(tags_with_weight.keys())\n",
    "        #weights = log_norm(list(tags_with_weight.values()))\n",
    "        weights = list(tags_with_weight.values())\n",
    "        #print(weights)\n",
    "        \n",
    "    else:\n",
    "        tags = tags_with_weight\n",
    "        weights = None\n",
    "        \n",
    "    embeddings = embed_model.encode(tags)\n",
    "    \n",
    "    silhouette_scores = []\n",
    "    distinct_tags_num = len(set(tags))\n",
    "    max_clusters = min(100, distinct_tags_num)  \n",
    "\n",
    "    for n_clusters in range(2, max_clusters - 1):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        kmeans.fit(embeddings, sample_weight=weights)\n",
    "        \n",
    "        if len(set(kmeans.labels_)) > 1:\n",
    "            score = silhouette_score(embeddings, kmeans.labels_)\n",
    "            silhouette_scores.append(score)\n",
    "        else:\n",
    "            silhouette_scores.append(-1)  \n",
    "    #print(silhouette_scores)\n",
    "    best_n_clusters = np.argmax(silhouette_scores) + 2 \n",
    "    best_score = silhouette_scores[best_n_clusters - 2]\n",
    "    \n",
    "    print(f\"best cluster number: {best_n_clusters} (Silhouette Coefficient: {best_score:.3f})\")\n",
    "\n",
    "    #best_n_clusters = 4\n",
    "    kmeans_best = KMeans(n_clusters=best_n_clusters, random_state=42)\n",
    "    y_kmeans = kmeans_best.fit_predict(embeddings, sample_weight=weights)\n",
    "    \n",
    "    cluster_centers = kmeans_best.cluster_centers_\n",
    "\n",
    "    clusters_info = []\n",
    "    \n",
    "    for i in range(best_n_clusters):\n",
    "        indices = np.where(kmeans_best.labels_ == i)[0]\n",
    "        \n",
    "        cluster_vectors = embeddings[indices]\n",
    "        cluster_tags = []\n",
    "        for vec in cluster_vectors:\n",
    "            cluster_tags.append(find_origin_tag(vec, tags, embeddings))\n",
    "        \n",
    "        center_vector = kmeans_best.cluster_centers_[i]\n",
    "        center_tag = find_origin_tag(center_vector, tags, embeddings)\n",
    "        \n",
    "        clusters_info.append({\n",
    "            'vectors': cluster_vectors,\n",
    "            \"all_tags\": cluster_tags,\n",
    "            'center_vector': center_vector,\n",
    "            \"center_tag\": center_tag\n",
    "        })\n",
    "\n",
    "    if visualize:\n",
    "        pca = PCA(n_components=2) \n",
    "        embeddings_pca = pca.fit_transform(embeddings)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        scatter = plt.scatter(embeddings_pca[:, 0], embeddings_pca[:, 1], \n",
    "                              c=y_kmeans, s=100, cmap='viridis', alpha=0.7, label='Cluster Labels')\n",
    "        \n",
    "        centers_pca = pca.transform(kmeans.cluster_centers_)  # 将聚类中心也降维\n",
    "        #plt.scatter(centers_pca[:, 0], centers_pca[:, 1], c='red', s=200, alpha=0.75, marker='X', label='Cluster Centers')\n",
    "        \n",
    "\n",
    "        cbar = plt.colorbar(scatter)\n",
    "        cbar.set_label('Cluster Label')\n",
    "        \n",
    "        plt.title('KMeans Clustering Visualization of Embeddings')\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    return clusters_info\n",
    "\n",
    "def filter_tags(input_list, min_times=2):\n",
    "    #合并相同元素\n",
    "    #input_list = [lemmatizer.lemmatize(word.lower(), pos='n')  for word in input_list]\n",
    "    count = Counter(input_list)\n",
    "    merged_list = [item for item in count if count[item] >= min_times]\n",
    "    print(f\"tags number:{len(merged_list)}\")\n",
    "    return merged_list\n",
    "\n",
    "def split_tags(merged_list, group_size=10):\n",
    "    return [merged_list[i:i + group_size] for i in range(0, len(merged_list), group_size)]\n",
    "    \n",
    "#docs_sum: {doc_path:summary, }\n",
    "#return: [entity_tag, ]\n",
    "def extract_entity_tags(doc_path, docs_sum):\n",
    "    doc_path = os.path.abspath(doc_path)\n",
    "    doc_summary = docs_sum[doc_path]\n",
    "    with open(doc_path, \"r\") as f:\n",
    "        doc_content = f.read()\n",
    "        #doc_content = \"\\n\".join(line for line in doc_content.splitlines() if line.strip() != \"\")\n",
    "    doc_title = os.path.basename(doc_path)\n",
    "    doc_info = (doc_title, doc_summary, doc_content)\n",
    "    context  = []\n",
    "\n",
    "    @parse_model_resp(resp_type=\"list\")\n",
    "    def parse_list(prompt, context):\n",
    "        return request_model(prompt, context)\n",
    "\n",
    "    @parse_model_resp(resp_type=\"str\")\n",
    "    def parse_str(prompt, context):\n",
    "        return request_model(prompt, context)\n",
    "        \n",
    "        \n",
    "    entity_extraction_prompt = f\"\"\"\n",
    "        ----------------Task Requirements-------------------\n",
    "        Next, a text and its summary will be provided. You need to refer to the summary and the content of the text to classify meaningful entities within the text. Note that you only need to return the type names of the summary, not the specific entity names, and the number of entity types should be kept to a minimum. Each entity type should be in singular form and begin with an uppercase letter.\n",
    "        \n",
    "        ---------------Output Format Requirements--------------------\n",
    "        Return a list containing all the class names in the following format, with the list being wrapped in ### markers and each type being a string marked with single quotes, formatted as follows:\n",
    "        ###\n",
    "        [Return type list]\n",
    "        ###\n",
    "        ---------------Current Input--------------------\n",
    "        Current input text summary:\n",
    "        《{doc_info[0]}》 : {doc_info[1]}\n",
    "        \n",
    "        Current input text content:\n",
    "        {doc_info[2]}\n",
    "        \"\"\"\n",
    "    \n",
    "    logger.info(entity_extraction_prompt)\n",
    "    \n",
    "    max_try = 10\n",
    "    first_try = True \n",
    "    while max_try>0:\n",
    "        try:\n",
    "            if first_try:\n",
    "                first_try = False \n",
    "                entity_types = parse_list(entity_extraction_prompt, context)\n",
    "            else: entity_types = parse_list(format_error_prompt, context)\n",
    "            assert isinstance(entity_types, list)\n",
    "            return list(set(entity_types))\n",
    "        except Exception as err:\n",
    "            err_msg = f\"err: {err}\"\n",
    "            logger.error(err_msg)\n",
    "            format_error_prompt = f\"\"\"\n",
    "                The returned object does not meet the format requirements, an issue was encountered while parsing the return content: {err_msg}.\n",
    "                \n",
    "                Please check and correct the previous return result. The requirements are as follows:\n",
    "                \n",
    "                The return format must be a list object marked with ### at both the beginning and end, where each element is a string marked with single quotes, and no quotes are allowed inside the element strings.\n",
    "                The entity types should be meaningful and concise.\n",
    "                Continue to modify the format of the returned object so that it satisfies the above format requirements, i.e., a list containing all the class names marked with ### at both the beginning and end.\n",
    "                \n",
    "                Note that you only need to provide the corrected result, without the process or code.               \n",
    "                \"\"\"\n",
    "            print(f\"error: {err_msg}, retry\")\n",
    "            max_try -= 1 \n",
    "\n",
    "    print(\"exceed max retry, return []\")\n",
    "    return []\n",
    "            \n",
    "\n",
    "#return: {doc_path:[entity_type, ], }\n",
    "def extract_KB_entity_tags(KB_dir, docs_sum):\n",
    "    KB_schema = {}\n",
    "    \n",
    "    total_files = 0\n",
    "    accessed_files = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(KB_dir):\n",
    "        dirnames[:] = [d for d in dirnames if not d.startswith('.')]\n",
    "        total_files += len(filenames)\n",
    "        \n",
    "    for root, dirs, files in os.walk(KB_dir):\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        for file in files:\n",
    "            try:\n",
    "                doc_path = os.path.abspath(os.path.join(root, file))\n",
    "                print(doc_path)\n",
    "                doc_entity_tags = extract_entity_tags(doc_path, docs_sum)\n",
    "                KB_schema[doc_path] = doc_entity_tags\n",
    "                accessed_files += 1\n",
    "                print(f\"{accessed_files}/{total_files}\")\n",
    "            except Exception as err:\n",
    "                logger.error(f\"error processing {doc_path}\")\n",
    "                print(err)\n",
    "                continue\n",
    "    return KB_schema\n",
    "\n",
    "#tag_schema: json {path:[tag, ], }\n",
    "#return:[{tag:definition， }, ]\n",
    "def comment_KB_entity_tag(tag_schema, ref_summary_path, ref_num=20, min_times=5):\n",
    "    @parse_model_resp(resp_type=\"str\")\n",
    "    def ask_comment(prompt):\n",
    "        return request_model(prompt)\n",
    "\n",
    "    @parse_model_resp(resp_type=\"dict\")\n",
    "    def refine_tags(context):\n",
    "        refine_prompt = \"\"\"\n",
    "            Check and correct the previous return result. The requirements are as follows:\n",
    "            \n",
    "            Pay attention to the background knowledge and consider the meanings represented by various types in the context of the provided background knowledge to improve the specificity of the descriptions and avoid being overly broad.\n",
    "            Merge entities of the same meaning or with a clear inclusion relationship. For entities A and B that have a clear inclusion relationship, where A includes B, merge them into the broader type A. For example, \"server\" and \"http server\" should be merged into \"server.\"\n",
    "            The return format must be a JSON object marked with ### at the beginning and end, where both keys and values are enclosed in double quotes. Each key is a type name, and the value is the definition of that type. However, there should be no extra quotes inside the key and value strings, or around the JSON object itself.\n",
    "            \"\"\"\n",
    "        return request_model(refine_prompt, context)\n",
    "        \n",
    "    full_entity_schema = []    \n",
    "    tags = []\n",
    "    processed_clusters = 0\n",
    "    for doc in tag_schema:\n",
    "        entity_tags = tag_schema[doc]\n",
    "        for tag in entity_tags:\n",
    "            tags.append(tag)\n",
    "            \n",
    "    tags = filter_tags(tags, min_times)  \n",
    "    print(tags)\n",
    "\n",
    "    clusters_info = kmeans_clust(tags, False, False)\n",
    "    for cluster in clusters_info:\n",
    "        print(f\"{processed_clusters}/{len(clusters_info)}\")\n",
    "        processed_clusters += 1\n",
    "        cluster_tag_groups = split_tags(cluster[\"all_tags\"], 3)\n",
    "        print(cluster_tag_groups)\n",
    "        for cluster_tags in cluster_tag_groups:\n",
    "            ref_sums = vector_db.query(str(cluster_tags), ref_num, embed_model, ref_summary_path)\n",
    "            ref_context = \"\"\n",
    "            for i, ref in enumerate(ref_sums):\n",
    "                #print(ref_path)\n",
    "                ref_context += f\"{i+1}. {ref} \\n\"\n",
    "    \n",
    "            \n",
    "            extraction_prompt = f\"\"\"\n",
    "                ---------------------------------Task Requirements----------------------------------\n",
    "                There is a series of entity types. First, correct any spelling errors, then replace types that have the same meaning or have a clear inclusion relationship with one common term, and briefly define each merged type in one sentence.\n",
    "                When defining, make sure to incorporate background knowledge as much as possible to improve the specificity of the descriptions and avoid being overly broad.\n",
    "                For entity types A and B that have a clear inclusion relationship where A includes B, merge them into the broader type A. For example, \"server\" and \"http server\" should be merged into \"server.\"\n",
    "                The input entity types are as follows: {cluster_tags}\n",
    "                \n",
    "                -----------------------------------Output Format Requirements------------------------------------\n",
    "                \n",
    "                Return a JSON object containing all merged class definitions in the following format, where both the key and the value are marked with single quotes. Special attention is needed: the key and value strings must not contain any quotes internally. The JSON object should be marked with ### at the beginning and end, and should follow the format below, but keep in mind to only reference the format of the example, and do not treat the example as input:\n",
    "                ###\n",
    "                {{\"entity_type\": \"description and definition\", }}\n",
    "                ###\n",
    "\n",
    "                -----------------------------------Example input------------------------------------\n",
    "                ['Receiver', 'Header', 'Message', 'Series', 'Protocol'] \n",
    "\n",
    "                -----------------------------------Example Output------------------------------------\n",
    "                ###{{'Receiver': 'A component in the Prometheus ecosystem that accepts and processes incoming metrics or alerts, often part of a monitoring or alerting system.', 'Header': 'A metadata section in a data packet or message that contains control information, used to manage the transmission and processing of metrics or alerts in Prometheus.', 'Message': 'A unit of communication in Prometheus that contains metrics, alerts, or other data, transmitted between components such as exporters, servers, and AlertManager.', 'Series': 'A sequence of timestamped data points in Prometheus, representing a time series used for monitoring and analysis, often associated with a metric and labeled dimensions.', 'Protocol': 'A set of rules and conventions in Prometheus that govern the format and transmission of data, such as the exposition format or remote-write protocol, ensuring reliable and efficient communication between components.'}}###\n",
    "                \n",
    "                -----------------------------------Background Knowledge-----------------------------------\n",
    "                \n",
    "                These entity types are part of the same system, for which there is documentation that may include domain introductions, terminology definitions, operational methods, and other content. Please merge and briefly define and describe each type using all available documentation:\n",
    "                {ref_context}\n",
    "            \"\"\"\n",
    "        \n",
    "            #print(cluster_tags, \"\\n\")\n",
    "            \n",
    "            try:\n",
    "                origin_resp = ask_comment(extraction_prompt)\n",
    "                context = [\n",
    "                    {\n",
    "                     \"role\": \"user\",\n",
    "                     \"content\": extraction_prompt\n",
    "                    },\n",
    "                    {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": origin_resp\n",
    "                }]\n",
    "                refined_resp = refine_tags(context)\n",
    "                full_entity_schema.append(refined_resp)\n",
    "                print(refined_resp)\n",
    "                \n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "                break\n",
    "    return full_entity_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7e899-edda-4e86-9d4d-9180866f28b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time.sleep(3)\n",
    "docs_sum = load_dict_from_file(f\"{OUTPUT_PATH}/docs_summary.json\")\n",
    "entity_tags = extract_KB_entity_tags(DATA_PATH, docs_sum)\n",
    "save_dict_to_file(entity_tags, f\"{OUTPUT_PATH}/entity_tags_bydoc.json\")\n",
    "len(load_dict_from_file(f\"{OUTPUT_PATH}/entity_tags_bydoc.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba42b6-9222-4b88-b05f-7f89ba21478c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entity_tags = load_dict_from_file(f\"{OUTPUT_PATH}/entity_tags_bydoc.json\")\n",
    "full_entity_schema = comment_KB_entity_tag(entity_tags, f\"{OUTPUT_PATH}/summary_faiss_index.bin\", min_times=5)\n",
    "save_dict_to_file(full_entity_schema, f\"{OUTPUT_PATH}/full_entity_schema.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff77bfc1-0397-4ac5-bc40-3fd2a1aa6d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "###entity extraction\n",
    "\n",
    "\n",
    "#return: {tag:[entities]}\n",
    "def extract_entity_full_schema(entity_schema, doc_path, docs_sum):\n",
    "    with open(doc_path, \"r\") as f:\n",
    "        doc_content = f.read().replace('\"', \" \").replace('“', \" \").replace('”', \" \")\n",
    "\n",
    "    doc_path = os.path.abspath(doc_path)\n",
    "    doc_summary = docs_sum[doc_path].replace('\"', \" \").replace('“', \" \").replace('”', \" \")\n",
    "    with open(doc_path, \"r\") as f:\n",
    "        doc_content = \"\\n\".join(line for line in doc_content.splitlines() if line.strip() != \"\")\n",
    "        \n",
    "    doc_title = os.path.basename(doc_path)\n",
    "    #doc_info = (doc_title, doc_summary, doc_content)\n",
    "    \n",
    "    @parse_model_resp(resp_type=\"original_str\")\n",
    "    def refine_entities_1(prompt, context):\n",
    "        return request_model(prompt, context)\n",
    "\n",
    "    @parse_model_resp(resp_type=\"dict\")\n",
    "    def refine_entities_2(prompt, context):\n",
    "        return request_model(prompt, context)\n",
    "\n",
    "    extraction_prompt = f\"\"\"\n",
    "        Here is a document. Please extract meaningful entities from the document based on their types in conjunction with the specific content, and explain the reasons.\n",
    "        \n",
    "        Requirements:\n",
    "        \n",
    "        1.Pay attention to reading the document summary and consider how the summary encapsulates the entities within the document.        \n",
    "        2.Entities should be as specific as possible, avoiding being too abstract or broad. And sufficiently rich in number.\n",
    "        3.Ensure the extracted entities do exist in the provided text(Detailed Content) but not Entity Type Description. \n",
    "        4.Extract entity All entities should be in singular form and capitalized.\n",
    "        5.Return in JSON format, where both keys and values are enclosed in double quotes. Each key is a type name, and the value contains entities of that type, separated by semicolons (;). However, double quotes are not allowed within the key and value strings.\n",
    "        6.Ensure that the extracted entities are consistent with the original text. If the original text is Chinese, the entities are expressed in Chinese. If the original text is in English, the entity is in English.\n",
    "        \n",
    "        Enclose the returned JSON object with ### markers at the beginning and end, following the format below. Note that you should only refer to the example format and not consider the example as input:\n",
    "        \n",
    "           ###\n",
    "           {{\"EntityType1\": \"Entity1;Entity2;\",  }}\n",
    "           ###\n",
    "\n",
    "        Document Summary:\n",
    "        {{\n",
    "        {doc_summary}\n",
    "        }}  \n",
    "        \n",
    "        Detailed Content:\n",
    "        {{\n",
    "        {doc_content}\n",
    "        }}  \n",
    "        \n",
    "        --------Entity Type Description-------------\n",
    "        The following are all possible types of entities and their descriptions. Please only extract entities that can be categorized into the following types. If there are none, return an empty dictionary. Note that you can only extract entities from the above document, not from the entity type descriptions.\n",
    "        {{\n",
    "        {entity_schema}\n",
    "        }}  \n",
    "    \"\"\"\n",
    "    \n",
    "    context = []\n",
    "    \n",
    "    origin_resp = request_model(extraction_prompt, context)\n",
    "\n",
    "    refine_prompt_1 = \"\"\"\n",
    "        Check and correct the previous classification results. The requirements are as follows:\n",
    "\n",
    "        1.Pay attention to reading the document summary and consider how the summary encapsulates the entities in the document.\n",
    "        2.Ensure the extracted entities do exist in the provided text but not from Entity Type Description. Delete the unexisted ones.\n",
    "        3.Check if there are any missing entities. If so, add the missing entities to the returned JSON object.\n",
    "        4.All entities should be in singular form and capitalized.\n",
    "        5.The return format must be a JSON object marked by ### at the beginning and end, with both keys and values marked with double quotes. Each key is a type name, and the value consists of entities under that type, separated by semicolons (;) within the same type and commas (,) between different types. Note that double quotes and parentheses are not allowed within key or value strings. \n",
    "        6.Ensure that the extracted entities are consistent with the original text. If the original text is Chinese, the entities are expressed in Chinese. If the original text is in English, the entity is in English.\n",
    "        \"\"\"\n",
    "\n",
    "    refined_resp1 = refine_entities_1(refine_prompt_1, context)\n",
    "    print(refined_resp1)\n",
    "\n",
    "    format_refine_context = []\n",
    "    max_try = 10\n",
    "    res = {}\n",
    "    while max_try>0:\n",
    "        try:\n",
    "            doc_entities = ast.literal_eval(refined_resp1)\n",
    "            for tag, entities in doc_entities.items():\n",
    "                if entities.strip() == \"\" : continue\n",
    "                entities_list = (entities.replace(\"；\", \";\")).split(\";\")\n",
    "                #英语数据加上\n",
    "                entities_list = [item for item in entities_list if (item in doc_content or item.lower() in doc_content) ]\n",
    "                if not entities_list: continue\n",
    "                print(f\"{tag}: {entities_list}\")\n",
    "                res[tag] = entities_list\n",
    "            return res\n",
    "        except Exception as err:\n",
    "            err_msg = f\"err: {err}\"\n",
    "            logger.error(err_msg)\n",
    "            format_error_prompt = f\"\"\"\n",
    "                The following object is parsed from the string: {refined_resp1}\n",
    "                The parsing reveals that the object does not meet the formatting requirements. The return format must be a JSON object marked with ### at the beginning and end, where both keys and values are enclosed in double quotes. Each key is a type name, and the value represents the entities under that type, with different entities separated by semicolons (;) and different types separated by commas (,). However, double quotes and parentheses are not allowed within the key and value strings.\n",
    "                Continue to modify the format of the object to meet the aforementioned formatting requirements.\n",
    "                Note that you only need to provide the corrected result, without any process or code.    \n",
    "                \"\"\"\n",
    "            refined_resp1 = refine_entities_1(format_error_prompt, format_refine_context)\n",
    "            print(f\"error: {err_msg}, retry\")\n",
    "            max_try -= 1 \n",
    "\n",
    "    print(\"exceed max retry, return {}\")\n",
    "    return {}\n",
    "\n",
    "#entity_schema: [{tag: comment,}, ]\n",
    "#return: {doc_path:{tag:[entity,], }, }\n",
    "def extract_KB_entities(entity_schema_path, docs_sum_path):\n",
    "    docs_sum = load_dict_from_file(docs_sum_path)\n",
    "    entity_schema = load_dict_from_file(entity_schema_path)\n",
    "    #print(entity_schema)\n",
    "    \n",
    "    doc_num = len(docs_sum)\n",
    "    processed_doc = 0\n",
    "    \n",
    "    all_entities = {}\n",
    "\n",
    "    for doc_path in docs_sum.keys():\n",
    "        all_entities[doc_path] = {}\n",
    "        try:\n",
    "            doc_entities = extract_entity_full_schema(entity_schema, doc_path, docs_sum)\n",
    "            all_entities[doc_path] = doc_entities\n",
    "            print(f\"{processed_doc+1}/{doc_num}: {doc_path} succeed\")\n",
    "        except Exception as err:\n",
    "            print(f\"{processed_doc+1}/{doc_num}: {err} processing {doc_path}\")\n",
    "            continue\n",
    "            \n",
    "        processed_doc += 1\n",
    "        #break\n",
    "    return all_entities  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21bce66-917e-4524-8264-d1b81b722382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_entities = extract_KB_entities(f\"{OUTPUT_PATH}/full_entity_schema.json\", f\"{OUTPUT_PATH}/docs_summary.json\")\n",
    "save_dict_to_file(all_entities, f\"{OUTPUT_PATH}/all_entities.json\")  \n",
    "len(load_dict_from_file(f\"{OUTPUT_PATH}/all_entities.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91464aab-2dbb-487c-a680-ab3bb4f68b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###triple extraction\n",
    "\n",
    "def split_text(content, chunk_size, lang=\"eng\"):\n",
    "    # sunfire, prometheus按行分割\n",
    "    chunks = content.strip().split(\".\")\n",
    "    chunks = [chunk.strip() for chunk in chunks]\n",
    "    \n",
    "    output_chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for part in chunks:\n",
    "        if part==\"\": continue\n",
    "        if lang==\"eng\":\n",
    "            tokens = word_tokenize(part)\n",
    "        elif lang==\"chn\":\n",
    "            tokens = jieba.lcut(part) \n",
    "        else: return\n",
    "        token_count = len(tokens)\n",
    "        \n",
    "        if current_tokens + token_count <= chunk_size:\n",
    "            current_chunk.append(part.strip()) \n",
    "            current_tokens += token_count\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                output_chunks.append('\\n'.join(current_chunk))\n",
    "            current_chunk = [part.strip()]\n",
    "            current_tokens = token_count\n",
    "\n",
    "    if current_chunk:\n",
    "        output_chunks.append('\\n'.join(current_chunk))\n",
    "    return output_chunks\n",
    "\n",
    "#entity_list: {tag:[entiies], }\n",
    "#return: [entiies, ]\n",
    "def filter_entity_by_text(entity_list, text, with_tag=False, entity_schema={}):\n",
    "    result = []\n",
    "    entity_schema = {k: v for d in entity_schema for k, v in d.items()}\n",
    "\n",
    "    for key, value_list in entity_list.items():\n",
    "        found_strings = set()\n",
    "        \n",
    "        for string in value_list:\n",
    "            if string in text or string.lower() in text:\n",
    "                found_strings.add(string)\n",
    "        \n",
    "        if found_strings:\n",
    "            if with_tag:\n",
    "                for entity in found_strings:\n",
    "                    try: result.append((entity, key, entity_schema[key]))\n",
    "                    except: continue\n",
    "            else:\n",
    "                result.extend(list(found_strings))\n",
    "    return set(result)\n",
    "\n",
    "\n",
    "#entity_schema:[{entity_type: entities}, ]\n",
    "#entity_list: (tag:[entiies], )\n",
    "#return: [(s:s_type, p, o:o_type), ]\n",
    "def extract_triple(entity_schema, entity_list, doc_path, chunk_size=200):\n",
    "    entity_tags = list(set(key for item in entity_schema for key in item.keys()))\n",
    "    with open(doc_path, \"r\") as f:\n",
    "        doc_content = f.read().replace('\"', \" \").replace('“', \" \").replace('”', \" \")\n",
    "\n",
    "    doc_path = os.path.abspath(doc_path)\n",
    "\n",
    "    @parse_model_resp(resp_type=\"original_str\")\n",
    "    def refine_entities(prompt, context, model=\"default\"):\n",
    "        if model == \"default\": return request_model(prompt, context)\n",
    "        else: return request_model(prompt, context, model=model)\n",
    "\n",
    "    triples = []\n",
    "    chunks = split_text(doc_content, chunk_size, lang=\"chn\")\n",
    "    for idx,text_chunk in enumerate(chunks):   \n",
    "        print(f\"chunk: {idx+1} / {len(chunks)}\")\n",
    "        logger.info(f\"{doc_path}, chunk {idx}, \\n content: \\n{text_chunk}\")\n",
    "        existing_entities = filter_entity_by_text(entity_list, text_chunk, False)\n",
    "        logger.info(f\"existing_entities: {existing_entities}\")\n",
    "        if existing_entities == {}: continue\n",
    "        extraction_prompt = f\"\"\"\n",
    "            A document is as follows:\n",
    "            \n",
    "            {{\n",
    "            {text_chunk}\n",
    "            }}\n",
    "            \n",
    "            Entities extracted from this document are as follows:\n",
    "            \n",
    "            {{\n",
    "            {existing_entities}\n",
    "            }}\n",
    "            \n",
    "            Based on the original text, what entities do you believe have clear and meaningful relationships? Provide the relationships between them with the following requirements:\n",
    "            \n",
    "            1.Use list(tuple) format, where each tuple is represented as a triplet (Subject-Relationship-Object). All entities and relationships should be in singular form with the first letter capitalized.\n",
    "            2.At least one of the subject or object in the tuple must come from the entities provided above or from the entities given above.\n",
    "            3.The types of relationships should be diversed, and the number of relationship tuples should be maximized.\n",
    "            4.The triplets should represent specific, clear, and meaningful relationships and should not include vague relationships like 'is related to'.\n",
    "            5.The relationship content (predicate) should be short and clear, expressing the semantic relationship in as few words as possible.\n",
    "                    \"\"\"\n",
    "        \n",
    "        context = []\n",
    "        \n",
    "        origin_resp = request_model(extraction_prompt, context)\n",
    "\n",
    "        refine_prompt_1 = f\"\"\"   \n",
    "            First, remove ambiguous relations like “has”, \"is\", \"related to\"\n",
    "            Then, annotate the types of all entities in all tuples, changing the format to (\"Subject Entity Type: Subject Entity\", \"Relationship Content\", \"Object Entity Type: Object Entity\") based on the original tuples. Each element in the tuple should be marked with double quotes before and after.\n",
    "            \n",
    "            Return a list object where each element is a modified tuple, with the entity types before the entities in both the subject and the object. All entities and relationships in the tuples should be in singular form, with the first letter capitalized.\n",
    "            \n",
    "            All entity types are as follows; please choose the most appropriate one from the list below and annotate it within the existing entities:\n",
    "            {{\n",
    "            {entity_tags}\n",
    "            }}\n",
    "        \"\"\"\n",
    "    \n",
    "        refined_resp1 = request_model(refine_prompt_1, context)\n",
    "        \n",
    "        #format_refine_context = []\n",
    "        #规范格式\n",
    "        format_refine_prompt= f\"\"\"\n",
    "            For the list object just mentioned, where each element is a tuple representing a relationship, the following conditions need to be checked and corrected, and then return the corrected result:\n",
    "            \n",
    "            1.For entities missing type annotations, label their types as required above, while keeping the subject-verb-object order unchanged.\n",
    "            2.Translate the Chinese in the tuples into English and correct any misspellings in the English words. All entities and relationships in the tuples should be in singular form, and their initial letters should be capitalized.\n",
    "            3.The tuple format has three elements。Both subject and object entities are prefixed with their type and separated by a colon : (\"Subject Entity Type: Subject Entity\", \"Relationship Content\", \"Object Entity Type: Object Entity\").\n",
    "            4.Each element in the tuple should be marked with double quotes \" at both ends, and there should be no other quotes. There should be no quotes within the elements or between the tuples; if there are any quotes within an element, remove them.\n",
    "            5.Return a list object marked with ### (three # symbols) at both ends, ensuring that the list has exactly one level of square brackets [] and no additional content. The format should be as follows: ###[(\"Subject Entity Type: Subject Entity\", \"Relationship Content\", \"Object Entity Type: Object Entity\"),]###. Note that the tuples are the content you need to fill in.\n",
    "            6.Ensure that the entities are consistent with the original text. If the original text is Chinese, the entities are expressed in Chinese. If the original text is in English, the entity is in English.\n",
    "                    \"\"\"\n",
    "        if origin_resp is None or refined_resp1 is None: continue\n",
    "\n",
    "        try:\n",
    "            refined_resp2 = refine_entities(format_refine_prompt, context)\n",
    "        except: continue\n",
    "            \n",
    "        #print(refined_resp2)\n",
    "        max_try = 3\n",
    "        while max_try>0:\n",
    "            try:\n",
    "                refined_resp2 = refined_resp2.replace(\"）\",\")\").replace(\"（\", \"(\").replace(\"：\", \":\").replace(\"，\", \",\")\n",
    "                ret = ast.literal_eval(refined_resp2)\n",
    "                assert isinstance(ret, list)\n",
    "                triples.extend(ret)\n",
    "                print(ret)\n",
    "                break\n",
    "            except Exception as err:\n",
    "                max_try -= 1 \n",
    "                err_msg = f\"err: {err} at {doc_path} chunk {idx}\"\n",
    "                logger.error(err_msg)\n",
    "                format_error_prompt = f\"\"\"\n",
    "                    From the returned content above, parsed out the following object: {refined_resp2}\n",
    "                    An error occurred while using ast.literal_eval to parse the returned object: {err_msg}\n",
    "                    \n",
    "                    Continue to modify the format of the object so that it meets the requirements of the above format and can be correctly parsed by ast.literal_eval.\n",
    "                    Note that you only need to provide the corrected result and mark it with three # symbols formatted like ###reutrned object###.\n",
    "                    \n",
    "                    Typical errors that may occur:\n",
    "                    1.Pay attention to whether each element in the tuple has double quotes; if so, they must be removed.\n",
    "                    2.Instead of using colons to separate entities and types, bracketed tags are incorrectly used\n",
    "                    3.Each tuple can only have 3 elements. The entity type and entity must be in the same element of the tuple (i.e., within the same string) and separated by a colon. It must conform to the format (\"subject entity type:subject entity\", \"relationship content\", \"object entity type:object entity\"). If the type and entity are found in different strings, they need to be combined into one.   \n",
    "                    4.The returned object may not be marked with ###\n",
    "                    5.Entities may miss type annotations, you need to label their types as required above, while keeping the subject-verb-object order unchanged.\"\"\"\n",
    "                \n",
    "                try:\n",
    "                    refined_resp2 = refine_entities(format_error_prompt, context)\n",
    "                except:\n",
    "                    print(\"resp parsing error\")\n",
    "                    continue\n",
    "                print(f\"error: {err_msg}, retry\")\n",
    "                if max_try==0: print(\"exceed max retry, continue\")\n",
    "\n",
    "    logger.info(f\"triples:\\n {triples}\")    \n",
    "\n",
    "    return triples\n",
    "\n",
    "#all_entities: {doc_path: {entity_tag: [entitiy, ], }, } \n",
    "def extract_KB_triples(entity_list_path, entity_schema_path, chunk_size=200):\n",
    "    all_entities = load_dict_from_file(entity_list_path)\n",
    "    entity_schema = load_dict_from_file(entity_schema_path)\n",
    "    #print(all_entities)\n",
    "    \n",
    "    doc_num = len(all_entities)\n",
    "    processed_doc = 0\n",
    "    \n",
    "    all_triples = {}\n",
    "\n",
    "    for doc_path in all_entities.keys():\n",
    "        print(f\"{processed_doc+1}/{doc_num}: {doc_path}\")\n",
    "        processed_doc += 1\n",
    "\n",
    "        entity_list = all_entities[doc_path]\n",
    "        if entity_list == {} : continue\n",
    "        doc_triples = extract_triple(entity_schema, entity_list, doc_path, chunk_size=chunk_size)\n",
    "        all_triples[doc_path] = doc_triples\n",
    "        #break\n",
    "    return all_triples  \n",
    "\n",
    "def post_process(triples_path, entity_schema_path):\n",
    "    triples = load_dict_from_file(triples_path)\n",
    "    entity_schema = load_dict_from_file(entity_schema_path)\n",
    "    entity_tags = list(set(key for item in entity_schema for key in item.keys()))\n",
    "\n",
    "    output_json = {}\n",
    "\n",
    "    for key, value in triples.items():\n",
    "        output_list = []\n",
    "        try:\n",
    "            for tup in value:\n",
    "                if len(tup) == 3 and \":\" in tup[0] and \":\" in tup[2]:\n",
    "                    s, p, o = tup\n",
    "                    s_tag, s_entity = s.split(':', 1)\n",
    "                    o_tag, o_entity = o.split(':', 1)\n",
    "    \n",
    "                    s_tag = find_best_match(s_tag, entity_tags)\n",
    "                    o_tag = find_best_match(o_tag, entity_tags)\n",
    "                    \n",
    "                    output_dict = {\n",
    "                        'subject_tag': s_tag,\n",
    "                        'subject_entity': s_entity.strip(),\n",
    "                        'predicate': p.strip(),\n",
    "                        'object_tag': o_tag,\n",
    "                        'object_entity': o_entity.strip()\n",
    "                    }\n",
    "                    output_list.append(output_dict)\n",
    "        except: continue\n",
    "        output_json[key] = output_list\n",
    "    save_dict_to_file(output_json, triples_path)\n",
    "    return output_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e9493-d4fe-4799-a4f6-d3c673bd9da6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 200\n",
    "all_triples = extract_KB_triples(f\"{OUTPUT_PATH}/all_entities.json\", f\"{OUTPUT_PATH}/full_entity_schema.json\", chunk_size=CHUNK_SIZE)\n",
    "save_dict_to_file(all_triples, f\"{OUTPUT_PATH}/all_triples.json\")\n",
    "post_process(f\"{OUTPUT_PATH}/all_triples.json\", f\"{OUTPUT_PATH}/full_entity_schema.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "graphrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
